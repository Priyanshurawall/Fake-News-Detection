# Fake News Detection System
# A comprehensive machine learning project to classify news articles as real or fake

import pandas as pd
import numpy as np
import re
import string
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
import warnings
warnings.filterwarnings('ignore')

# Natural Language Processing
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize

# Machine Learning
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
from sklearn.pipeline import Pipeline

# Download required NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')
    
try:
    nltk.data.find('corpora/stopwords')
except LookupError:
    nltk.download('stopwords')

class FakeNewsDetector:
    def __init__(self):
        self.stemmer = PorterStemmer()
        self.stop_words = set(stopwords.words('english'))
        self.vectorizer = None
        self.model = None
        
    def preprocess_text(self, text):
        """
        Comprehensive text preprocessing function
        """
        if pd.isna(text):
            return ""
        
        # Convert to lowercase
        text = text.lower()
        
        # Remove URLs
        text = re.sub(r'http\S+|www\S+|https\S+', '', text, flags=re.MULTILINE)
        
        # Remove user mentions and hashtags
        text = re.sub(r'@\w+|#\w+', '', text)
        
        # Remove punctuation and numbers
        text = re.sub(r'[^a-zA-Z\s]', '', text)
        
        # Tokenization
        tokens = word_tokenize(text)
        
        # Remove stopwords and stem
        tokens = [self.stemmer.stem(token) for token in tokens 
                 if token not in self.stop_words and len(token) > 2]
        
        return ' '.join(tokens)
    
    def load_and_prepare_data(self, fake_path=None, real_path=None):
        """
        Load and prepare the dataset
        If no paths provided, creates sample data for demonstration
        """
        if fake_path and real_path:
            # Load real datasets
            fake_df = pd.read_csv("Fake.csv")
            real_df = pd.read_csv("True.csv")
            
            fake_df['label'] = 1  # Fake news
            real_df['label'] = 0  # Real news
            
            # Combine datasets
            df = pd.concat([fake_df, real_df], ignore_index=True)
        else:
            # Create sample data for demonstration
            fake_news_samples = [
                "Scientists discover aliens living on Mars, government covers up evidence",
                "Miracle cure for cancer found in common kitchen ingredient, doctors hate this",
                "Celebrity admits to being part of secret society controlling world economy",
                "Local man discovers shocking truth about water that will change everything",
                "Breaking: President secretly replaced by robot double, insider reveals",
                "Amazing weight loss secret discovered by mom, pharmaceutical companies furious",
                "Shocking revelation about vaccines that mainstream media won't tell you",
                "Ancient artifact proves existence of time travel, archaeologists baffled",
                "Celebrity death hoax spreads across social media platforms rapidly today",
                "Conspiracy theory about moon landing gains traction among internet users"
            ]
            
            real_news_samples = [
                "Local election results show increased voter turnout in urban areas nationwide",
                "New study reveals benefits of regular exercise for mental health improvement",
                "Technology company announces quarterly earnings exceeding analyst expectations significantly",
                "Weather service issues storm warning for coastal regions this weekend",
                "University researchers publish findings on climate change effects in journal",
                "Stock market shows mixed results following federal reserve interest announcement",
                "Healthcare workers receive recognition for outstanding service during pandemic response",
                "Infrastructure bill passes committee review, moves to full legislative body",
                "Scientists develop new method for early disease detection using technology",
                "International trade agreement signed between multiple countries for cooperation"
            ]
            
            # Create DataFrame
            fake_df = pd.DataFrame({
                'text': fake_news_samples * 50,  # Multiply for more data
                'label': [1] * len(fake_news_samples) * 50
            })
            
            real_df = pd.DataFrame({
                'text': real_news_samples * 50,
                'label': [0] * len(real_news_samples) * 50
            })
            
            df = pd.concat([fake_df, real_df], ignore_index=True)
            
        # Shuffle the dataset
        df = df.sample(frac=1).reset_index(drop=True)
        
        print(f"Dataset loaded: {len(df)} articles")
        print(f"Fake news: {sum(df['label'])}")
        print(f"Real news: {len(df) - sum(df['label'])}")
        
        return df
    
    def exploratory_data_analysis(self, df):
        """
        Perform exploratory data analysis
        """
        print("\n=== Exploratory Data Analysis ===")
        
        # Basic statistics
        print(f"Total articles: {len(df)}")
        print(f"Label distribution:\n{df['label'].value_counts()}")
        
        # Text length analysis
        df['text_length'] = df['text'].str.len()
        
        plt.figure(figsize=(12, 4))
        
        plt.subplot(1, 2, 1)
        sns.histplot(data=df, x='text_length', hue='label', bins=30)
        plt.title('Text Length Distribution')
        plt.xlabel('Character Count')
        
        plt.subplot(1, 2, 2)
        sns.boxplot(data=df, x='label', y='text_length')
        plt.title('Text Length by Label')
        plt.xlabel('Label (0=Real, 1=Fake)')
        
        plt.tight_layout()
        plt.show()
        
        # Word clouds
        fake_text = ' '.join(df[df['label'] == 1]['text'].astype(str))
        real_text = ' '.join(df[df['label'] == 0]['text'].astype(str))
        
        plt.figure(figsize=(15, 6))
        
        plt.subplot(1, 2, 1)
        wordcloud_fake = WordCloud(width=400, height=300, background_color='white').generate(fake_text)
        plt.imshow(wordcloud_fake, interpolation='bilinear')
        plt.title('Fake News Word Cloud')
        plt.axis('off')
        
        plt.subplot(1, 2, 2)
        wordcloud_real = WordCloud(width=400, height=300, background_color='white').generate(real_text)
        plt.imshow(wordcloud_real, interpolation='bilinear')
        plt.title('Real News Word Cloud')
        plt.axis('off')
        
        plt.tight_layout()
        plt.show()
    
    def train_models(self, df):
        """
        Train multiple machine learning models and compare performance
        """
        print("\n=== Training Models ===")
        
        # Preprocess text
        print("Preprocessing text...")
        df['processed_text'] = df['text'].apply(self.preprocess_text)
        
        # Prepare features and labels
        X = df['processed_text']
        y = df['label']
        
        # Split data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        # Define models to test
        models = {
            'Logistic Regression': LogisticRegression(random_state=42),
            'Naive Bayes': MultinomialNB(),
            'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'SVM': SVC(random_state=42)
        }
        
        # Define vectorizers to test
        vectorizers = {
            'TF-IDF': TfidfVectorizer(max_features=5000, ngram_range=(1, 2)),
            'Count': CountVectorizer(max_features=5000, ngram_range=(1, 2))
        }
        
        results = []
        best_score = 0
        best_pipeline = None
        
        print("Testing different combinations...")
        
        for vec_name, vectorizer in vectorizers.items():
            for model_name, model in models.items():
                # Create pipeline
                pipeline = Pipeline([
                    ('vectorizer', vectorizer),
                    ('classifier', model)
                ])
                
                # Cross-validation
                cv_scores = cross_val_score(pipeline, X_train, y_train, cv=5, scoring='accuracy')
                mean_score = cv_scores.mean()
                
                # Train on full training set and test
                pipeline.fit(X_train, y_train)
                test_score = pipeline.score(X_test, y_test)
                
                results.append({
                    'Vectorizer': vec_name,
                    'Model': model_name,
                    'CV Score': mean_score,
                    'Test Score': test_score
                })
                
                # Track best model
                if test_score > best_score:
                    best_score = test_score
                    best_pipeline = pipeline
                    self.vectorizer = vectorizer
                    self.model = model
                
                print(f"{vec_name} + {model_name}: CV={mean_score:.4f}, Test={test_score:.4f}")
        
        # Results summary
        results_df = pd.DataFrame(results)
        print("\n=== Model Comparison ===")
        print(results_df.sort_values('Test Score', ascending=False))
        
        # Detailed evaluation of best model
        print(f"\n=== Best Model Performance ===")
        print(f"Best combination: {results_df.loc[results_df['Test Score'].idxmax(), 'Vectorizer']} + {results_df.loc[results_df['Test Score'].idxmax(), 'Model']}")
        print(f"Test Accuracy: {best_score:.4f}")
        
        # Predictions and detailed metrics
        y_pred = best_pipeline.predict(X_test)
        print("\nClassification Report:")
        print(classification_report(y_test, y_pred, target_names=['Real', 'Fake']))
        
        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)
        plt.figure(figsize=(8, 6))
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                   xticklabels=['Real', 'Fake'], yticklabels=['Real', 'Fake'])
        plt.title('Confusion Matrix - Best Model')
        plt.ylabel('True Label')
        plt.xlabel('Predicted Label')
        plt.show()
        
        # Store the best pipeline
        self.best_pipeline = best_pipeline
        
        return results_df
    
    def predict_news(self, text_list):
        """
        Predict whether news articles are fake or real
        """
        if not hasattr(self, 'best_pipeline'):
            print("Model not trained yet. Please train the model first.")
            return None
        
        if isinstance(text_list, str):
            text_list = [text_list]
        
        # Preprocess texts
        processed_texts = [self.preprocess_text(text) for text in text_list]
        
        # Make predictions
        predictions = self.best_pipeline.predict(processed_texts)
        probabilities = self.best_pipeline.predict_proba(processed_texts)
        
        results = []
        for i, (text, pred, prob) in enumerate(zip(text_list, predictions, probabilities)):
            result = {
                'text': text[:100] + '...' if len(text) > 100 else text,
                'prediction': 'FAKE' if pred == 1 else 'REAL',
                'confidence': max(prob),
                'fake_probability': prob[1] if len(prob) > 1 else prob[0]
            }
            results.append(result)
        
        return results
    
    def feature_importance_analysis(self, df, top_n=20):
        """
        Analyze feature importance for interpretability
        """
        if not hasattr(self, 'best_pipeline'):
            print("Model not trained yet. Please train the model first.")
            return
        
        print(f"\n=== Feature Importance Analysis (Top {top_n}) ===")
        
        # Get feature names from vectorizer
        vectorizer = self.best_pipeline.named_steps['vectorizer']
        classifier = self.best_pipeline.named_steps['classifier']
        
        feature_names = vectorizer.get_feature_names_out()
        
        # Different methods based on classifier type
        if hasattr(classifier, 'coef_'):
            # For linear models (Logistic Regression, SVM)
            importance = classifier.coef_[0]
            
            # Get top features for fake news (positive coefficients)
            fake_indices = importance.argsort()[-top_n:][::-1]
            fake_features = [(feature_names[i], importance[i]) for i in fake_indices]
            
            # Get top features for real news (negative coefficients)
            real_indices = importance.argsort()[:top_n]
            real_features = [(feature_names[i], abs(importance[i])) for i in real_indices]
            
            print("\nTop words indicating FAKE news:")
            for feature, coef in fake_features:
                print(f"  {feature}: {coef:.4f}")
            
            print("\nTop words indicating REAL news:")
            for feature, coef in real_features:
                print(f"  {feature}: {coef:.4f}")
                
        elif hasattr(classifier, 'feature_importances_'):
            # For tree-based models (Random Forest)
            importance = classifier.feature_importances_
            top_indices = importance.argsort()[-top_n:][::-1]
            
            print("Top important features:")
            for i in top_indices:
                print(f"  {feature_names[i]}: {importance[i]:.4f}")

def main():
    """
    Main function to demonstrate the fake news detection system
    """
    print("=== Fake News Detection System ===")
    
    # Initialize detector
    detector = FakeNewsDetector()
    
    # Load and prepare data
    # For real datasets, use: df = detector.load_and_prepare_data('fake.csv', 'real.csv')
    df = detector.load_and_prepare_data()
    
    # Perform EDA
    detector.exploratory_data_analysis(df)
    
    # Train models
    results = detector.train_models(df)
    
    # Feature importance analysis
    detector.feature_importance_analysis(df)
    
    # Example predictions
    print("\n=== Example Predictions ===")
    test_articles = [
        "Scientists announce breakthrough in cancer research with new treatment method",
        "Miracle cure discovered by grandmother using kitchen ingredients doctors hate",
        "Local government announces new infrastructure development project for community",
        "Celebrity spotted with aliens at secret government facility last night"
    ]
    
    predictions = detector.predict_news(test_articles)
    for result in predictions:
        print(f"\nText: {result['text']}")
        print(f"Prediction: {result['prediction']}")
        print(f"Confidence: {result['confidence']:.4f}")
        print(f"Fake Probability: {result['fake_probability']:.4f}")

if __name__ == "__main__":
    main()

# Usage Instructions:
"""
1. Install required packages:
   pip install pandas numpy scikit-learn nltk matplotlib seaborn wordcloud

2. For real datasets, you can use:
   - Kaggle fake news datasets
   - Reuters news corpus
   - Any CSV files with 'text' and 'label' columns

3. To use with your own data:
   detector = FakeNewsDetector()
   df = detector.load_and_prepare_data('path_to_fake.csv', 'path_to_real.csv')
   detector.train_models(df)
   predictions = detector.predict_news(['Your news text here'])

4. The system includes:
   - Text preprocessing and cleaning
   - Multiple ML algorithms comparison
   - Feature extraction using TF-IDF and Count Vectorization
   - Model evaluation and visualization
   - Prediction functionality with confidence scores
   - Feature importance analysis for interpretability
"""